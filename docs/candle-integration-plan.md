# Candle MLæ¡†æ¶é›†æˆæ–¹æ¡ˆ

## ğŸ“‹ æ–‡æ¡£ä¿¡æ¯

- **æ–‡æ¡£ç‰ˆæœ¬**: 1.0.0
- **åˆ›å»ºæ—¥æœŸ**: 2024-01-XX
- **æœ€åæ›´æ–°**: 2024-01-XX
- **ä½œè€…**: Edge Compute Team
- **çŠ¶æ€**: ç”Ÿäº§å¯ç”¨æ–¹æ¡ˆ

---

## ğŸ¯ æ‰§è¡Œæ‘˜è¦

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°äº†å°† **Candle MLæ¡†æ¶** é›†æˆåˆ° **Rustè¾¹ç¼˜è®¡ç®—æ¡†æ¶** çš„å®Œæ•´æ–¹æ¡ˆã€‚Candleæ˜¯Hugging Faceå¼€å‘çš„è½»é‡çº§Rustæœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒCPU/GPUæ¨ç†ã€å¤šç§æ¨¡å‹æ ¼å¼ï¼Œéå¸¸é€‚åˆè¾¹ç¼˜è®¡ç®—åœºæ™¯ã€‚

### é›†æˆç›®æ ‡

1. **MLç®—æ³•æ‰§è¡Œå¼•æ“**: å°†Candleä½œä¸ºæœºå™¨å­¦ä¹ ç®—æ³•æ‰§è¡Œå¼•æ“
2. **æ¨¡å‹æ¨ç†æœåŠ¡**: æ”¯æŒLLMã€CVã€éŸ³é¢‘ç­‰å¤šç§æ¨¡å‹æ¨ç†
3. **è¾¹ç¼˜AIèƒ½åŠ›**: åœ¨è¾¹ç¼˜èŠ‚ç‚¹æä¾›AIæ¨ç†èƒ½åŠ›
4. **ç»Ÿä¸€ä»»åŠ¡è°ƒåº¦**: ä¸ç°æœ‰ä»»åŠ¡è°ƒåº¦ç³»ç»Ÿæ— ç¼é›†æˆ
5. **å®¹å™¨åŒ–éƒ¨ç½²**: æ”¯æŒæ¨¡å‹å®¹å™¨åŒ–éƒ¨ç½²å’Œç®¡ç†

### æ ¸å¿ƒä»·å€¼

- âœ… **è½»é‡çº§éƒ¨ç½²**: Candleç¼–è¯‘åä½“ç§¯å°ï¼Œé€‚åˆè¾¹ç¼˜è®¾å¤‡
- âœ… **é«˜æ€§èƒ½æ¨ç†**: æ”¯æŒCUDA/MetalåŠ é€Ÿï¼Œæ€§èƒ½ä¼˜å¼‚
- âœ… **æ¨¡å‹ç”Ÿæ€**: æ”¯æŒHugging Faceæ¨¡å‹åº“ï¼Œæ¨¡å‹ä¸°å¯Œ
- âœ… **RuståŸç”Ÿ**: ä¸ç°æœ‰Rustæ¡†æ¶å®Œç¾é›†æˆï¼Œæ— FFIå¼€é”€
- âœ… **ç”Ÿäº§å°±ç»ª**: å·²åœ¨å¤šä¸ªç”Ÿäº§ç¯å¢ƒéªŒè¯

---

## ğŸ“Š ç°çŠ¶åˆ†æ

### å½“å‰ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å®¢æˆ·ç«¯å±‚                                â”‚
â”‚         (Web/ç§»åŠ¨/API/ç‰©è”ç½‘è®¾å¤‡)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ§åˆ¶å¹³é¢ (Control Plane)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ HTTP API â”‚  â”‚  è®¤è¯æˆæƒ â”‚  â”‚ é€Ÿç‡é™åˆ¶ â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              è°ƒåº¦å±‚ (Scheduler Layer)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ä»»åŠ¡è°ƒåº¦å™¨ â”‚  â”‚å·¥ä½œçº¿ç¨‹æ±  â”‚  â”‚ä¼˜å…ˆçº§è°ƒåº¦ â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              æ‰§è¡Œå±‚ (Execution Layer)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ FFIæ¡¥æ¥  â”‚  â”‚å®¹å™¨è¿è¡Œæ—¶ â”‚  â”‚ C++ç®—æ³•  â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Candleæ¡†æ¶èƒ½åŠ›

#### 1. æ ¸å¿ƒç»„ä»¶

- **candle-core**: æ ¸å¿ƒå¼ é‡æ“ä½œã€è®¾å¤‡ç®¡ç†
- **candle-nn**: ç¥ç»ç½‘ç»œå±‚ã€ä¼˜åŒ–å™¨
- **candle-transformers**: Transformeræ¨¡å‹æ”¯æŒ
- **candle-examples**: ä¸°å¯Œçš„ç¤ºä¾‹ä»£ç 
- **candle-onnx**: ONNXæ¨¡å‹æ”¯æŒ

#### 2. æ”¯æŒçš„æ¨¡å‹ç±»å‹

- **è¯­è¨€æ¨¡å‹**: LLaMAã€Mistralã€Phiã€Gemmaã€Qwenç­‰
- **è§†è§‰æ¨¡å‹**: YOLOã€Segment Anythingã€CLIPã€DINOv2ç­‰
- **éŸ³é¢‘æ¨¡å‹**: Whisperã€EnCodecã€MetaVoiceç­‰
- **å¤šæ¨¡æ€æ¨¡å‹**: BLIPã€LLaVAã€Moondreamç­‰

#### 3. è®¾å¤‡æ”¯æŒ

- **CPU**: æ”¯æŒMKL/Accelerateä¼˜åŒ–
- **CUDA**: GPUåŠ é€Ÿæ¨ç†
- **Metal**: Apple Silicon GPUæ”¯æŒ
- **WASM**: æµè§ˆå™¨ç«¯æ¨ç†

### é›†æˆæŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

| æŒ‘æˆ˜ | è§£å†³æ–¹æ¡ˆ |
|------|---------|
| æ¨¡å‹åŠ è½½æ—¶é—´é•¿ | æ¨¡å‹é¢„åŠ è½½ã€æ¨¡å‹ç¼“å­˜æ±  |
| å†…å­˜å ç”¨å¤§ | æ¨¡å‹é‡åŒ–ã€åŠ¨æ€åŠ è½½å¸è½½ |
| GPUèµ„æºç«äº‰ | GPUèµ„æºæ± ã€ä»»åŠ¡é˜Ÿåˆ— |
| æ¨¡å‹ç‰ˆæœ¬ç®¡ç† | æ¨¡å‹æ³¨å†Œè¡¨ã€ç‰ˆæœ¬æ§åˆ¶ |
| é”™è¯¯å¤„ç†å¤æ‚ | ç»Ÿä¸€é”™è¯¯å¤„ç†ã€é‡è¯•æœºåˆ¶ |

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      å®¢æˆ·ç«¯å±‚                                 â”‚
â”‚              (REST API / WebSocket / gRPC)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APIç½‘å…³å±‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  è·¯ç”±åˆ†å‘    â”‚  â”‚  è®¤è¯æˆæƒ    â”‚  â”‚  é€Ÿç‡é™åˆ¶    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ä»»åŠ¡è°ƒåº¦å±‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           ç»Ÿä¸€ä»»åŠ¡è°ƒåº¦å™¨ (TaskScheduler)              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚    â”‚
â”‚  â”‚  â”‚ä¼ ç»Ÿç®—æ³•  â”‚  â”‚ MLæ¨ç†   â”‚  â”‚å®¹å™¨åŒ–    â”‚         â”‚    â”‚
â”‚  â”‚  â”‚ä»»åŠ¡é˜Ÿåˆ—  â”‚  â”‚ä»»åŠ¡é˜Ÿåˆ—  â”‚  â”‚ä»»åŠ¡é˜Ÿåˆ—  â”‚         â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  C++ç®—æ³•æ‰§è¡Œ â”‚ â”‚ Candle MLæ‰§è¡Œ â”‚ â”‚ å®¹å™¨åŒ–æ‰§è¡Œ   â”‚
â”‚   (FFI)      â”‚ â”‚   (Native)    â”‚ â”‚  (Youki)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒæ¨¡å—è®¾è®¡

#### 1. Candleæ‰§è¡Œå¼•æ“æ¨¡å—

```rust
// src/ml/candle_executor.rs

pub struct CandleExecutor {
    /// è®¾å¤‡ç®¡ç†å™¨
    device_manager: Arc<DeviceManager>,
    /// æ¨¡å‹æ³¨å†Œè¡¨
    model_registry: Arc<RwLock<ModelRegistry>>,
    /// æ¨¡å‹ç¼“å­˜æ± 
    model_cache: Arc<RwLock<ModelCache>>,
    /// GPUèµ„æºæ± 
    gpu_pool: Option<Arc<GpuResourcePool>>,
    /// æ‰§è¡Œç»Ÿè®¡
    stats: Arc<RwLock<ExecutionStats>>,
}

pub struct ModelRegistry {
    /// æ¨¡å‹ä¿¡æ¯æ˜ å°„
    models: HashMap<String, ModelInfo>,
    /// æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
    versions: HashMap<String, Vec<ModelVersion>>,
}

pub struct ModelInfo {
    pub name: String,
    pub version: String,
    pub model_type: ModelType,
    pub model_path: PathBuf,
    pub config_path: Option<PathBuf>,
    pub tokenizer_path: Option<PathBuf>,
    pub device: Device,
    pub resource_requirements: ResourceRequirements,
}
```

#### 2. æ¨¡å‹ç®¡ç†å™¨æ¨¡å—

```rust
// src/ml/model_manager.rs

pub struct ModelManager {
    /// æ¨¡å‹åŠ è½½å™¨
    loader: Arc<ModelLoader>,
    /// æ¨¡å‹ç¼“å­˜ç­–ç•¥
    cache_strategy: CacheStrategy,
    /// é¢„åŠ è½½é…ç½®
    preload_config: PreloadConfig,
}

pub enum ModelType {
    /// è¯­è¨€æ¨¡å‹
    LanguageModel(LanguageModelType),
    /// è§†è§‰æ¨¡å‹
    VisionModel(VisionModelType),
    /// éŸ³é¢‘æ¨¡å‹
    AudioModel(AudioModelType),
    /// å¤šæ¨¡æ€æ¨¡å‹
    MultimodalModel(MultimodalModelType),
}

pub enum LanguageModelType {
    Llama,
    Mistral,
    Phi,
    Gemma,
    Qwen,
    // ... æ›´å¤šæ¨¡å‹
}
```

#### 3. æ¨ç†æœåŠ¡æ¨¡å—

```rust
// src/ml/inference_service.rs

pub struct InferenceService {
    executor: Arc<CandleExecutor>,
    scheduler: Arc<TaskScheduler>,
}

pub enum InferenceRequest {
    /// æ–‡æœ¬ç”Ÿæˆ
    TextGeneration {
        model: String,
        prompt: String,
        max_tokens: Option<usize>,
        temperature: Option<f64>,
    },
    /// å›¾åƒåˆ†ç±»
    ImageClassification {
        model: String,
        image: Vec<u8>,
    },
    /// è¯­éŸ³è¯†åˆ«
    SpeechRecognition {
        model: String,
        audio: Vec<u8>,
    },
    /// å¤šæ¨¡æ€æ¨ç†
    Multimodal {
        model: String,
        inputs: MultimodalInputs,
    },
}
```

---

## ğŸ”§ æŠ€æœ¯å®ç°æ–¹æ¡ˆ

### 1. ä¾èµ–é›†æˆ

#### Cargo.toml é…ç½®

```toml
[dependencies]
# Candleæ ¸å¿ƒåº“
candle-core = { path = "./candle/candle/candle-core", version = "0.9.2-alpha.1" }
candle-nn = { path = "./candle/candle/candle-nn", version = "0.9.2-alpha.1" }
candle-transformers = { path = "./candle/candle/candle-transformers", version = "0.9.2-alpha.1" }
candle-datasets = { path = "./candle/candle/candle-datasets", version = "0.9.2-alpha.1" }

# å¯é€‰ç‰¹æ€§
[features]
default = []
# CUDAæ”¯æŒ
cuda = ["candle-core/cuda", "candle-nn/cuda"]
# cuDNNæ”¯æŒï¼ˆéœ€è¦CUDAï¼‰
cudnn = ["cuda", "candle-core/cudnn"]
# MKLä¼˜åŒ–ï¼ˆIntel CPUï¼‰
mkl = ["candle-core/mkl"]
# Accelerateä¼˜åŒ–ï¼ˆApple Siliconï¼‰
accelerate = ["candle-core/accelerate"]
# Metalæ”¯æŒï¼ˆApple GPUï¼‰
metal = ["candle-core/metal"]
# ONNXæ”¯æŒ
onnx = ["candle-onnx"]

# ç°æœ‰ä¾èµ–ä¿æŒä¸å˜
tokio = { version = "1.0", features = ["full"] }
axum = "0.7"
serde = { version = "1.0", features = ["derive"] }
# ... å…¶ä»–ä¾èµ–
```

### 2. æ¨¡å—ç»“æ„

```
src/
â”œâ”€â”€ ml/                          # MLæ¨¡å—ï¼ˆæ–°å¢ï¼‰
â”‚   â”œâ”€â”€ mod.rs                   # æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ candle_executor.rs       # Candleæ‰§è¡Œå¼•æ“
â”‚   â”œâ”€â”€ model_manager.rs         # æ¨¡å‹ç®¡ç†å™¨
â”‚   â”œâ”€â”€ model_registry.rs        # æ¨¡å‹æ³¨å†Œè¡¨
â”‚   â”œâ”€â”€ model_cache.rs           # æ¨¡å‹ç¼“å­˜
â”‚   â”œâ”€â”€ inference_service.rs     # æ¨ç†æœåŠ¡
â”‚   â”œâ”€â”€ device_manager.rs        # è®¾å¤‡ç®¡ç†
â”‚   â”œâ”€â”€ gpu_pool.rs              # GPUèµ„æºæ± 
â”‚   â”œâ”€â”€ types.rs                 # MLç±»å‹å®šä¹‰
â”‚   â””â”€â”€ error.rs                 # MLé”™è¯¯å¤„ç†
â”œâ”€â”€ core/                        # æ ¸å¿ƒæ¨¡å—ï¼ˆæ‰©å±•ï¼‰
â”‚   â”œâ”€â”€ scheduler.rs             # æ‰©å±•æ”¯æŒMLä»»åŠ¡
â”‚   â””â”€â”€ types.rs                 # æ‰©å±•ComputeRequestæ”¯æŒML
â”œâ”€â”€ api/                         # APIæ¨¡å—ï¼ˆæ‰©å±•ï¼‰
â”‚   â”œâ”€â”€ handlers.rs              # æ‰©å±•ML APIå¤„ç†å™¨
â”‚   â””â”€â”€ routes.rs                # æ‰©å±•MLè·¯ç”±
â””â”€â”€ config/                      # é…ç½®æ¨¡å—ï¼ˆæ‰©å±•ï¼‰
    â””â”€â”€ settings.rs              # æ‰©å±•MLé…ç½®
```

### 3. æ ¸å¿ƒå®ç°ä»£ç 

#### 3.1 Candleæ‰§è¡Œå¼•æ“

```rust
// src/ml/candle_executor.rs

use candle_core::{Device, Tensor, DType};
use candle_nn::VarBuilder;
use std::sync::Arc;
use tokio::sync::RwLock;
use std::collections::HashMap;
use std::path::PathBuf;

pub struct CandleExecutor {
    device_manager: Arc<DeviceManager>,
    model_registry: Arc<RwLock<ModelRegistry>>,
    model_cache: Arc<RwLock<ModelCache>>,
    gpu_pool: Option<Arc<GpuResourcePool>>,
}

impl CandleExecutor {
    pub fn new(config: CandleConfig) -> Result<Self> {
        // åˆå§‹åŒ–è®¾å¤‡ç®¡ç†å™¨
        let device_manager = Arc::new(DeviceManager::new(config.device_config)?);
        
        // åˆå§‹åŒ–æ¨¡å‹æ³¨å†Œè¡¨
        let model_registry = Arc::new(RwLock::new(ModelRegistry::new()));
        
        // åˆå§‹åŒ–æ¨¡å‹ç¼“å­˜
        let model_cache = Arc::new(RwLock::new(ModelCache::new(
            config.cache_config
        )?));
        
        // åˆå§‹åŒ–GPUèµ„æºæ± ï¼ˆå¦‚æœå¯ç”¨CUDAï¼‰
        let gpu_pool = if config.enable_gpu {
            Some(Arc::new(GpuResourcePool::new(config.gpu_config)?))
        } else {
            None
        };
        
        Ok(Self {
            device_manager,
            model_registry,
            model_cache,
            gpu_pool,
        })
    }
    
    /// æ‰§è¡Œæ¨ç†ä»»åŠ¡
    pub async fn execute_inference(
        &self,
        request: InferenceRequest,
    ) -> Result<InferenceResponse> {
        // 1. éªŒè¯è¯·æ±‚
        self.validate_request(&request)?;
        
        // 2. è·å–æˆ–åŠ è½½æ¨¡å‹
        let model = self.get_or_load_model(&request.model_name()).await?;
        
        // 3. å‡†å¤‡è¾“å…¥æ•°æ®
        let inputs = self.prepare_inputs(&request).await?;
        
        // 4. æ‰§è¡Œæ¨ç†
        let outputs = self.run_inference(&model, inputs).await?;
        
        // 5. åå¤„ç†ç»“æœ
        let response = self.postprocess_outputs(outputs, &request).await?;
        
        Ok(response)
    }
    
    /// åŠ è½½æ¨¡å‹
    async fn load_model(&self, model_info: &ModelInfo) -> Result<LoadedModel> {
        // æ£€æŸ¥ç¼“å­˜
        if let Some(cached) = self.model_cache.read().await.get(&model_info.name) {
            return Ok(cached.clone());
        }
        
        // è·å–è®¾å¤‡
        let device = self.device_manager.get_device(&model_info.device)?;
        
        // åŠ è½½æ¨¡å‹æƒé‡
        let weights = self.load_weights(&model_info.model_path, &device).await?;
        
        // æ„å»ºæ¨¡å‹
        let model = self.build_model(model_info, weights)?;
        
        // åŠ è½½tokenizerï¼ˆå¦‚æœéœ€è¦ï¼‰
        let tokenizer = if let Some(path) = &model_info.tokenizer_path {
            Some(self.load_tokenizer(path).await?)
        } else {
            None
        };
        
        let loaded_model = LoadedModel {
            model,
            tokenizer,
            device,
            model_info: model_info.clone(),
        };
        
        // ç¼“å­˜æ¨¡å‹
        self.model_cache.write().await.insert(
            model_info.name.clone(),
            loaded_model.clone(),
        );
        
        Ok(loaded_model)
    }
}
```

#### 3.2 æ¨¡å‹ç®¡ç†å™¨

```rust
// src/ml/model_manager.rs

pub struct ModelManager {
    loader: Arc<ModelLoader>,
    cache_strategy: CacheStrategy,
    preload_config: PreloadConfig,
}

impl ModelManager {
    /// æ³¨å†Œæ¨¡å‹
    pub async fn register_model(
        &self,
        info: ModelInfo,
    ) -> Result<()> {
        // éªŒè¯æ¨¡å‹æ–‡ä»¶
        self.validate_model_files(&info)?;
        
        // æ³¨å†Œåˆ°æ³¨å†Œè¡¨
        self.registry.write().await.register(info.clone())?;
        
        // å¦‚æœé…ç½®äº†é¢„åŠ è½½ï¼Œåˆ™é¢„åŠ è½½æ¨¡å‹
        if self.preload_config.enabled && 
           self.preload_config.models.contains(&info.name) {
            self.preload_model(&info.name).await?;
        }
        
        Ok(())
    }
    
    /// é¢„åŠ è½½æ¨¡å‹
    async fn preload_model(&self, model_name: &str) -> Result<()> {
        let info = self.registry.read().await.get(model_name)?;
        let _ = self.executor.load_model(&info).await?;
        Ok(())
    }
}
```

#### 3.3 æ¨ç†æœåŠ¡é›†æˆ

```rust
// src/ml/inference_service.rs

pub struct InferenceService {
    executor: Arc<CandleExecutor>,
    scheduler: Arc<TaskScheduler>,
}

impl InferenceService {
    /// æäº¤æ¨ç†ä»»åŠ¡
    pub async fn submit_inference(
        &self,
        request: InferenceRequest,
    ) -> Result<String> {
        // åˆ›å»ºMLä»»åŠ¡
        let task = ScheduledTask::new(
            ComputeRequest {
                id: uuid::Uuid::new_v4().to_string(),
                algorithm: format!("ml:{}", request.model_name()),
                parameters: serde_json::to_value(&request)?,
                timeout_seconds: Some(self.get_timeout(&request)),
            }
        )
        .with_priority(self.get_priority(&request))
        .with_max_retries(1); // MLä»»åŠ¡é€šå¸¸ä¸é‡è¯•
        
        // æäº¤åˆ°è°ƒåº¦å™¨
        let task_id = self.scheduler.submit_task(task).await?;
        
        Ok(task_id)
    }
}
```

### 4. APIæ‰©å±•

#### 4.1 ML APIè·¯ç”±

```rust
// src/api/routes.rs

pub fn create_ml_routes() -> Router<AppState> {
    Router::new()
        .route("/api/v1/ml/models", get(list_models))
        .route("/api/v1/ml/models/:name", get(get_model_info))
        .route("/api/v1/ml/models/:name", post(register_model))
        .route("/api/v1/ml/models/:name", delete(unregister_model))
        .route("/api/v1/ml/inference/text", post(text_generation))
        .route("/api/v1/ml/inference/image", post(image_classification))
        .route("/api/v1/ml/inference/audio", post(speech_recognition))
        .route("/api/v1/ml/inference/multimodal", post(multimodal_inference))
        .route("/api/v1/ml/device/status", get(device_status))
        .route("/api/v1/ml/stats", get(inference_stats))
}
```

#### 4.2 ML APIå¤„ç†å™¨

```rust
// src/api/ml_handlers.rs

/// æ–‡æœ¬ç”ŸæˆAPI
pub async fn text_generation(
    state: State<AppState>,
    Json(request): Json<TextGenerationRequest>,
) -> Response {
    let inference_request = InferenceRequest::TextGeneration {
        model: request.model,
        prompt: request.prompt,
        max_tokens: request.max_tokens,
        temperature: request.temperature,
    };
    
    match state.ml_service.submit_inference(inference_request).await {
        Ok(task_id) => {
            (StatusCode::ACCEPTED, Json(json!({
                "task_id": task_id,
                "status": "submitted"
            }))).into_response()
        }
        Err(e) => {
            (StatusCode::INTERNAL_SERVER_ERROR, 
             Json(json!({"error": e.to_string()}))).into_response()
        }
    }
}
```

### 5. é…ç½®æ‰©å±•

#### 5.1 é…ç½®æ–‡ä»¶æ‰©å±•

```toml
# config/default.toml

[ml]
# å¯ç”¨MLåŠŸèƒ½
enabled = true
# é»˜è®¤è®¾å¤‡ç±»å‹: "cpu", "cuda", "metal"
default_device = "cpu"
# å¯ç”¨GPUæ”¯æŒ
enable_gpu = false
# GPUè®¾å¤‡IDï¼ˆå¦‚æœå¯ç”¨CUDAï¼‰
gpu_device_id = 0

[ml.model_cache]
# ç¼“å­˜ç­–ç•¥: "lru", "fifo", "none"
strategy = "lru"
# æœ€å¤§ç¼“å­˜æ¨¡å‹æ•°
max_models = 5
# ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
ttl_seconds = 3600

[ml.preload]
# å¯ç”¨é¢„åŠ è½½
enabled = false
# é¢„åŠ è½½æ¨¡å‹åˆ—è¡¨
models = []

[ml.resource_limits]
# æœ€å¤§å¹¶å‘æ¨ç†ä»»åŠ¡æ•°
max_concurrent_inference = 3
# é»˜è®¤æ¨ç†è¶…æ—¶ï¼ˆç§’ï¼‰
default_timeout_seconds = 300
# æœ€å¤§å†…å­˜ä½¿ç”¨ï¼ˆMBï¼‰
max_memory_mb = 2048

[ml.models]
# æ¨¡å‹å­˜å‚¨æ ¹ç›®å½•
model_dir = "./models"
# è‡ªåŠ¨æ‰«ææ¨¡å‹ç›®å½•
auto_scan = true
# æ‰«æé—´éš”ï¼ˆç§’ï¼‰
scan_interval_seconds = 60
```

---

## ğŸš€ å®æ–½è®¡åˆ’

### Phase 1: åŸºç¡€é›†æˆ (2-3å‘¨)

#### 1.1 ä¾èµ–é›†æˆ
- [x] æ·»åŠ Candleä¾èµ–åˆ°Cargo.toml
- [ ] é…ç½®ç¼–è¯‘ç‰¹æ€§ï¼ˆCPU/CUDA/Metalï¼‰
- [ ] è§£å†³ä¾èµ–å†²çª
- [ ] éªŒè¯ç¼–è¯‘é€šè¿‡

#### 1.2 æ ¸å¿ƒæ¨¡å—å¼€å‘
- [ ] å®ç°CandleExecutoråŸºç¡€ç»“æ„
- [ ] å®ç°DeviceManagerè®¾å¤‡ç®¡ç†
- [ ] å®ç°ModelRegistryæ¨¡å‹æ³¨å†Œè¡¨
- [ ] å®ç°åŸºç¡€æ¨¡å‹åŠ è½½åŠŸèƒ½

#### 1.3 æµ‹è¯•éªŒè¯
- [ ] å•å…ƒæµ‹è¯•ï¼šè®¾å¤‡ç®¡ç†
- [ ] å•å…ƒæµ‹è¯•ï¼šæ¨¡å‹åŠ è½½
- [ ] é›†æˆæµ‹è¯•ï¼šç«¯åˆ°ç«¯æ¨ç†

**äº¤ä»˜ç‰©**:
- åŸºç¡€Candleæ‰§è¡Œå¼•æ“
- æ¨¡å‹åŠ è½½åŠŸèƒ½
- å•å…ƒæµ‹è¯•å¥—ä»¶

### Phase 2: æ¨ç†æœåŠ¡ (3-4å‘¨)

#### 2.1 æ¨ç†æœåŠ¡å®ç°
- [ ] å®ç°æ–‡æœ¬ç”Ÿæˆæ¨ç†
- [ ] å®ç°å›¾åƒåˆ†ç±»æ¨ç†
- [ ] å®ç°è¯­éŸ³è¯†åˆ«æ¨ç†
- [ ] å®ç°å¤šæ¨¡æ€æ¨ç†

#### 2.2 ä»»åŠ¡è°ƒåº¦é›†æˆ
- [ ] æ‰©å±•TaskScheduleræ”¯æŒMLä»»åŠ¡
- [ ] å®ç°MLä»»åŠ¡ä¼˜å…ˆçº§è°ƒåº¦
- [ ] å®ç°GPUèµ„æºæ± ç®¡ç†
- [ ] å®ç°ä»»åŠ¡è¶…æ—¶å’Œå–æ¶ˆ

#### 2.3 APIå¼€å‘
- [ ] å®ç°ML APIè·¯ç”±
- [ ] å®ç°ML APIå¤„ç†å™¨
- [ ] å®ç°APIæ–‡æ¡£
- [ ] å®ç°APIæµ‹è¯•

**äº¤ä»˜ç‰©**:
- å®Œæ•´æ¨ç†æœåŠ¡
- ML APIæ¥å£
- APIæ–‡æ¡£

### Phase 3: ä¼˜åŒ–ä¸ç”Ÿäº§åŒ– (3-4å‘¨)

#### 3.1 æ€§èƒ½ä¼˜åŒ–
- [ ] å®ç°æ¨¡å‹ç¼“å­˜ç­–ç•¥
- [ ] å®ç°æ¨¡å‹é¢„åŠ è½½
- [ ] å®ç°æ‰¹å¤„ç†æ¨ç†
- [ ] å®ç°GPUèµ„æºæ± 
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

#### 3.2 ç”Ÿäº§ç‰¹æ€§
- [ ] å®ç°æ¨¡å‹ç‰ˆæœ¬ç®¡ç†
- [ ] å®ç°æ¨¡å‹çƒ­æ›´æ–°
- [ ] å®ç°é”™è¯¯æ¢å¤æœºåˆ¶
- [ ] å®ç°ç›‘æ§å’ŒæŒ‡æ ‡
- [ ] å®ç°æ—¥å¿—è®°å½•

#### 3.3 æ–‡æ¡£å’Œæµ‹è¯•
- [ ] ç¼–å†™é›†æˆæ–‡æ¡£
- [ ] ç¼–å†™APIæ–‡æ¡£
- [ ] ç¼–å†™éƒ¨ç½²æŒ‡å—
- [ ] ç¼–å†™æ€§èƒ½è°ƒä¼˜æŒ‡å—
- [ ] å®Œæ•´æµ‹è¯•å¥—ä»¶

**äº¤ä»˜ç‰©**:
- ç”Ÿäº§å°±ç»ªçš„MLæœåŠ¡
- å®Œæ•´æ–‡æ¡£
- æ€§èƒ½æŠ¥å‘Š

### Phase 4: é«˜çº§ç‰¹æ€§ (å¯é€‰, 4-6å‘¨)

#### 4.1 æ¨¡å‹ç®¡ç†
- [ ] å®ç°æ¨¡å‹è‡ªåŠ¨ä¸‹è½½
- [ ] å®ç°æ¨¡å‹è½¬æ¢å·¥å…·
- [ ] å®ç°æ¨¡å‹é‡åŒ–æ”¯æŒ
- [ ] å®ç°æ¨¡å‹A/Bæµ‹è¯•

#### 4.2 é«˜çº§æ¨ç†
- [ ] å®ç°æµå¼æ¨ç†
- [ ] å®ç°æ‰¹é‡æ¨ç†
- [ ] å®ç°æ¨ç†ç®¡é“
- [ ] å®ç°æ¨¡å‹é›†æˆ

#### 4.3 ç›‘æ§å’Œè¿ç»´
- [ ] å®ç°æ¨ç†æŒ‡æ ‡ç›‘æ§
- [ ] å®ç°æ¨¡å‹æ€§èƒ½åˆ†æ
- [ ] å®ç°è‡ªåŠ¨æ‰©ç¼©å®¹
- [ ] å®ç°æ•…éšœè‡ªæ„ˆ

**äº¤ä»˜ç‰©**:
- é«˜çº§MLåŠŸèƒ½
- è¿ç»´å·¥å…·
- ç›‘æ§ä»ªè¡¨æ¿

---

## ğŸ“ è¯¦ç»†è®¾è®¡

### 1. è®¾å¤‡ç®¡ç†

```rust
pub struct DeviceManager {
    default_device: Device,
    available_devices: Vec<DeviceInfo>,
    device_pool: HashMap<Device, Arc<DevicePool>>,
}

impl DeviceManager {
    /// è·å–å¯ç”¨è®¾å¤‡
    pub fn get_device(&self, device_type: &str) -> Result<Device> {
        match device_type {
            "cpu" => Ok(Device::Cpu),
            "cuda" => {
                Device::new_cuda(0)
                    .map_err(|e| format!("CUDA device not available: {}", e))
            }
            "metal" => {
                Device::new_metal(0)
                    .map_err(|e| format!("Metal device not available: {}", e))
            }
            _ => Err("Unknown device type".into()),
        }
    }
    
    /// æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
    pub fn check_device_availability(&self) -> DeviceStatus {
        // æ£€æŸ¥CPU
        let cpu_available = true;
        
        // æ£€æŸ¥CUDA
        let cuda_available = Device::new_cuda(0).is_ok();
        
        // æ£€æŸ¥Metal
        let metal_available = Device::new_metal(0).is_ok();
        
        DeviceStatus {
            cpu: cpu_available,
            cuda: cuda_available,
            metal: metal_available,
        }
    }
}
```

### 2. æ¨¡å‹ç¼“å­˜

```rust
pub struct ModelCache {
    cache: HashMap<String, CachedModel>,
    strategy: CacheStrategy,
    max_size: usize,
    ttl: Duration,
}

impl ModelCache {
    /// LRUç¼“å­˜ç­–ç•¥
    pub fn get_lru(&mut self, key: &str) -> Option<&CachedModel> {
        // å®ç°LRUé€»è¾‘
        self.cache.get(key)
    }
    
    /// æ’å…¥ç¼“å­˜
    pub fn insert(&mut self, key: String, model: LoadedModel) {
        // å¦‚æœè¶…è¿‡æœ€å¤§å¤§å°ï¼Œç§»é™¤æœ€æ—§çš„
        if self.cache.len() >= self.max_size {
            self.evict_oldest();
        }
        
        self.cache.insert(key, CachedModel {
            model,
            last_accessed: Instant::now(),
        });
    }
    
    /// æ¸…ç†è¿‡æœŸç¼“å­˜
    pub fn cleanup_expired(&mut self) {
        let now = Instant::now();
        self.cache.retain(|_, cached| {
            now.duration_since(cached.last_accessed) < self.ttl
        });
    }
}
```

### 3. GPUèµ„æºæ± 

```rust
pub struct GpuResourcePool {
    devices: Vec<GpuDevice>,
    available_slots: Vec<usize>,
    task_assignments: HashMap<String, usize>,
}

impl GpuResourcePool {
    /// åˆ†é…GPUèµ„æº
    pub async fn allocate(
        &self,
        task_id: &str,
        requirements: ResourceRequirements,
    ) -> Result<GpuAllocation> {
        // æŸ¥æ‰¾å¯ç”¨GPU
        for (idx, device) in self.devices.iter().enumerate() {
            if device.has_capacity(&requirements) {
                let allocation = GpuAllocation {
                    device_id: idx,
                    device: device.clone(),
                };
                
                // è®°å½•åˆ†é…
                self.task_assignments.insert(
                    task_id.to_string(),
                    idx,
                );
                
                return Ok(allocation);
            }
        }
        
        Err("No available GPU resources".into())
    }
    
    /// é‡Šæ”¾GPUèµ„æº
    pub async fn deallocate(&self, task_id: &str) -> Result<()> {
        self.task_assignments.remove(task_id);
        Ok(())
    }
}
```

### 4. é”™è¯¯å¤„ç†

```rust
#[derive(Debug, thiserror::Error)]
pub enum MlError {
    #[error("Model not found: {0}")]
    ModelNotFound(String),
    
    #[error("Model loading failed: {0}")]
    ModelLoadError(String),
    
    #[error("Inference failed: {0}")]
    InferenceError(String),
    
    #[error("Device not available: {0}")]
    DeviceNotAvailable(String),
    
    #[error("GPU resource exhausted")]
    GpuResourceExhausted,
    
    #[error("Invalid input: {0}")]
    InvalidInput(String),
    
    #[error("Timeout: {0}")]
    Timeout(String),
}
```

---

## ğŸ”’ å®‰å…¨è€ƒè™‘

### 1. æ¨¡å‹å®‰å…¨

- **æ¨¡å‹éªŒè¯**: éªŒè¯æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§ï¼ˆSHA256æ ¡éªŒï¼‰
- **æ¨¡å‹éš”ç¦»**: æ¯ä¸ªæ¨¡å‹åœ¨ç‹¬ç«‹ç¯å¢ƒä¸­è¿è¡Œ
- **è¾“å…¥éªŒè¯**: ä¸¥æ ¼éªŒè¯è¾“å…¥æ•°æ®ï¼Œé˜²æ­¢æ³¨å…¥æ”»å‡»
- **è¾“å‡ºè¿‡æ»¤**: è¿‡æ»¤æ•æ„Ÿä¿¡æ¯ï¼Œé˜²æ­¢æ•°æ®æ³„éœ²

### 2. èµ„æºå®‰å…¨

- **èµ„æºé™åˆ¶**: é™åˆ¶æ¯ä¸ªä»»åŠ¡çš„CPU/å†…å­˜/GPUä½¿ç”¨
- **è¶…æ—¶æ§åˆ¶**: è®¾ç½®æ¨ç†è¶…æ—¶ï¼Œé˜²æ­¢èµ„æºè€—å°½
- **å¹¶å‘æ§åˆ¶**: é™åˆ¶å¹¶å‘æ¨ç†ä»»åŠ¡æ•°
- **èµ„æºéš”ç¦»**: ä½¿ç”¨å®¹å™¨éš”ç¦»æ¨¡å‹æ‰§è¡Œç¯å¢ƒ

### 3. è®¿é—®æ§åˆ¶

- **APIè®¤è¯**: æ‰€æœ‰ML APIéœ€è¦JWTè®¤è¯
- **æƒé™æ§åˆ¶**: åŸºäºè§’è‰²çš„æ¨¡å‹è®¿é—®æ§åˆ¶
- **å®¡è®¡æ—¥å¿—**: è®°å½•æ‰€æœ‰æ¨¡å‹æ“ä½œ
- **é€Ÿç‡é™åˆ¶**: é™åˆ¶æ¨ç†è¯·æ±‚é¢‘ç‡

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æ¨¡å‹ä¼˜åŒ–

- **æ¨¡å‹é‡åŒ–**: ä½¿ç”¨INT8/INT4é‡åŒ–å‡å°‘å†…å­˜
- **æ¨¡å‹å‰ªæ**: ç§»é™¤ä¸é‡è¦çš„æ¨¡å‹å‚æ•°
- **æ‰¹å¤„ç†**: æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚
- **æ¨¡å‹ç¼“å­˜**: ç¼“å­˜å¸¸ç”¨æ¨¡å‹åˆ°å†…å­˜

### 2. æ¨ç†ä¼˜åŒ–

- **GPUåŠ é€Ÿ**: ä½¿ç”¨CUDA/MetalåŠ é€Ÿæ¨ç†
- **å¼‚æ­¥æ¨ç†**: å¼‚æ­¥æ‰§è¡Œæ¨ç†ä»»åŠ¡
- **æµæ°´çº¿**: å®ç°æ¨ç†æµæ°´çº¿å¹¶è¡Œ
- **é¢„åŠ è½½**: é¢„åŠ è½½å¸¸ç”¨æ¨¡å‹

### 3. ç³»ç»Ÿä¼˜åŒ–

- **è¿æ¥æ± **: å¤ç”¨æ¨¡å‹è¿æ¥
- **å†…å­˜æ± **: å¤ç”¨å¼ é‡å†…å­˜
- **ä»»åŠ¡è°ƒåº¦**: æ™ºèƒ½ä»»åŠ¡è°ƒåº¦ä¼˜åŒ–
- **è´Ÿè½½å‡è¡¡**: å¤šGPUè´Ÿè½½å‡è¡¡

### æ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ |
|------|--------|
| æ¨¡å‹åŠ è½½æ—¶é—´ | < 5s (ç¼“å­˜) / < 30s (é¦–æ¬¡) |
| æ¨ç†å»¶è¿Ÿ (P50) | < 100ms (å°æ¨¡å‹) / < 1s (å¤§æ¨¡å‹) |
| ååé‡ | > 100 req/s (CPU) / > 500 req/s (GPU) |
| å†…å­˜ä½¿ç”¨ | < 2GB (å•æ¨¡å‹) |
| GPUåˆ©ç”¨ç‡ | > 80% |

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•

```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_device_manager() {
        let manager = DeviceManager::new(Default::default()).unwrap();
        let device = manager.get_device("cpu").unwrap();
        assert_eq!(device, Device::Cpu);
    }
    
    #[tokio::test]
    async fn test_model_loading() {
        let executor = CandleExecutor::new(Default::default()).unwrap();
        // æµ‹è¯•æ¨¡å‹åŠ è½½
    }
    
    #[tokio::test]
    async fn test_inference() {
        // æµ‹è¯•æ¨ç†åŠŸèƒ½
    }
}
```

### 2. é›†æˆæµ‹è¯•

```rust
#[tokio::test]
async fn test_ml_api_integration() {
    // å¯åŠ¨æµ‹è¯•æœåŠ¡å™¨
    let app = create_test_app().await;
    
    // æµ‹è¯•æ¨¡å‹æ³¨å†Œ
    let response = app.post("/api/v1/ml/models")
        .json(&model_info)
        .send()
        .await
        .unwrap();
    assert_eq!(response.status(), 200);
    
    // æµ‹è¯•æ¨ç†
    let response = app.post("/api/v1/ml/inference/text")
        .json(&inference_request)
        .send()
        .await
        .unwrap();
    assert_eq!(response.status(), 202);
}
```

### 3. æ€§èƒ½æµ‹è¯•

```rust
#[tokio::test]
async fn test_inference_performance() {
    let executor = CandleExecutor::new(Default::default()).unwrap();
    
    let start = Instant::now();
    for _ in 0..100 {
        executor.execute_inference(request.clone()).await.unwrap();
    }
    let duration = start.elapsed();
    
    let avg_latency = duration / 100;
    assert!(avg_latency < Duration::from_millis(100));
}
```

### 4. å‹åŠ›æµ‹è¯•

- ä½¿ç”¨wrk/abè¿›è¡ŒAPIå‹åŠ›æµ‹è¯•
- æµ‹è¯•å¹¶å‘æ¨ç†ä»»åŠ¡
- æµ‹è¯•GPUèµ„æºç«äº‰
- æµ‹è¯•å†…å­˜æ³„æ¼

---

## ğŸ“š ä½¿ç”¨ç¤ºä¾‹

### 1. æ¨¡å‹æ³¨å†Œ

```bash
curl -X POST http://localhost:3000/api/v1/ml/models \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -d '{
    "name": "llama-7b",
    "version": "1.0",
    "model_type": "LanguageModel",
    "model_path": "/models/llama-7b.safetensors",
    "tokenizer_path": "/models/llama-7b-tokenizer.json",
    "device": "cuda",
    "resource_requirements": {
      "cpu_cores": 2.0,
      "memory_mb": 4096,
      "gpu_memory_mb": 8192
    }
  }'
```

### 2. æ–‡æœ¬ç”Ÿæˆ

```bash
curl -X POST http://localhost:3000/api/v1/ml/inference/text \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -d '{
    "model": "llama-7b",
    "prompt": "The future of AI is",
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

### 3. å›¾åƒåˆ†ç±»

```bash
curl -X POST http://localhost:3000/api/v1/ml/inference/image \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -d '{
    "model": "yolo-v8",
    "image": "base64_encoded_image_data"
  }'
```

### 4. Rustä»£ç ç¤ºä¾‹

```rust
use rust_edge_compute::ml::{CandleExecutor, InferenceRequest};

#[tokio::main]
async fn main() -> Result<()> {
    // åˆ›å»ºæ‰§è¡Œå™¨
    let executor = CandleExecutor::new(Default::default())?;
    
    // æ‰§è¡Œæ–‡æœ¬ç”Ÿæˆ
    let request = InferenceRequest::TextGeneration {
        model: "llama-7b".to_string(),
        prompt: "Hello, world!".to_string(),
        max_tokens: Some(50),
        temperature: Some(0.7),
    };
    
    let response = executor.execute_inference(request).await?;
    println!("Generated text: {}", response.text);
    
    Ok(())
}
```

---

## ğŸš¢ éƒ¨ç½²æ–¹æ¡ˆ

### 1. å¼€å‘ç¯å¢ƒéƒ¨ç½²

```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository>
cd rust-edge-compute

# 2. å®‰è£…ä¾èµ–
cargo build

# 3. ä¸‹è½½æ¨¡å‹ï¼ˆç¤ºä¾‹ï¼‰
mkdir -p models
# ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°modelsç›®å½•

# 4. è¿è¡ŒæœåŠ¡
cargo run --release
```

### 2. Dockeréƒ¨ç½²

```dockerfile
# Dockerfile.ml
FROM rust:1.75 as builder

WORKDIR /app

# å¤åˆ¶Candleæºç 
COPY candle/ ./candle/

# å¤åˆ¶é¡¹ç›®ä»£ç 
COPY . .

# æ„å»ºï¼ˆå¯ç”¨CUDAæ”¯æŒï¼‰
RUN cargo build --release --features cuda

FROM ubuntu:22.04

# å®‰è£…CUDAè¿è¡Œæ—¶ï¼ˆå¦‚æœéœ€è¦ï¼‰
# COPY --from=nvidia/cuda:12.0.0-runtime-ubuntu22.04 /usr/local/cuda /usr/local/cuda

# å¤åˆ¶äºŒè¿›åˆ¶æ–‡ä»¶
COPY --from=builder /app/target/release/rust-edge-compute /usr/local/bin/

# å¤åˆ¶æ¨¡å‹ç›®å½•
COPY models/ /models/

# è¿è¡Œ
CMD ["rust-edge-compute"]
```

```bash
# æ„å»ºé•œåƒ
docker build -f Dockerfile.ml -t rust-edge-compute:ml .

# è¿è¡Œå®¹å™¨
docker run -d \
  -p 3000:3000 \
  -v $(pwd)/models:/models \
  --gpus all \
  rust-edge-compute:ml
```

### 3. Kuberneteséƒ¨ç½²

```yaml
# k8s/ml-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rust-edge-compute-ml
spec:
  replicas: 2
  selector:
    matchLabels:
      app: rust-edge-compute-ml
  template:
    metadata:
      labels:
        app: rust-edge-compute-ml
    spec:
      containers:
      - name: rust-edge-compute
        image: rust-edge-compute:ml
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
            nvidia.com/gpu: 1
          limits:
            memory: "8Gi"
            cpu: "4"
            nvidia.com/gpu: 1
        volumeMounts:
        - name: models
          mountPath: /models
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: models-pvc
```

### 4. æ¨¡å‹å­˜å‚¨

- **æœ¬åœ°å­˜å‚¨**: é€‚ç”¨äºå•æœºéƒ¨ç½²
- **NFSå­˜å‚¨**: é€‚ç”¨äºå¤šèŠ‚ç‚¹å…±äº«
- **å¯¹è±¡å­˜å‚¨**: é€‚ç”¨äºäº‘éƒ¨ç½²ï¼ˆS3/MinIOï¼‰
- **æ¨¡å‹æ³¨å†Œè¡¨**: ä½¿ç”¨Hugging Face Hubæˆ–ç§æœ‰æ³¨å†Œè¡¨

---

## ğŸ“ˆ ç›‘æ§å’Œè¿ç»´

### 1. æŒ‡æ ‡ç›‘æ§

```rust
// PrometheusæŒ‡æ ‡
pub struct MlMetrics {
    // æ¨ç†æŒ‡æ ‡
    inference_requests_total: Counter,
    inference_duration_seconds: Histogram,
    inference_errors_total: Counter,
    
    // æ¨¡å‹æŒ‡æ ‡
    model_loads_total: Counter,
    model_cache_hits_total: Counter,
    model_cache_misses_total: Counter,
    
    // èµ„æºæŒ‡æ ‡
    gpu_utilization: Gauge,
    gpu_memory_used: Gauge,
    cpu_utilization: Gauge,
    memory_used: Gauge,
}
```

### 2. æ—¥å¿—è®°å½•

```rust
// ç»“æ„åŒ–æ—¥å¿—
tracing::info!(
    model = %model_name,
    device = %device,
    latency_ms = latency.as_millis(),
    "Inference completed"
);
```

### 3. å‘Šè­¦è§„åˆ™

```yaml
# prometheus/alerts.yml
groups:
  - name: ml_alerts
    rules:
      - alert: HighInferenceLatency
        expr: ml_inference_duration_seconds{quantile="0.95"} > 1
        for: 5m
        annotations:
          summary: "High inference latency detected"
      
      - alert: ModelLoadFailure
        expr: rate(ml_model_loads_failed_total[5m]) > 0.1
        annotations:
          summary: "Model loading failures detected"
      
      - alert: GpuResourceExhausted
        expr: ml_gpu_utilization > 0.95
        for: 10m
        annotations:
          summary: "GPU resources exhausted"
```

---

## ğŸ”„ è¿ç§»è®¡åˆ’

### ä»ç°æœ‰ç³»ç»Ÿè¿ç§»

1. **æ¸è¿›å¼è¿ç§»**
   - Phase 1: å¹¶è¡Œè¿è¡Œï¼ŒéªŒè¯åŠŸèƒ½
   - Phase 2: é€æ­¥åˆ‡æ¢æµé‡
   - Phase 3: å®Œå…¨åˆ‡æ¢ï¼Œä¸‹çº¿æ—§ç³»ç»Ÿ

2. **æ•°æ®è¿ç§»**
   - è¿ç§»æ¨¡å‹æ–‡ä»¶
   - è¿ç§»é…ç½®æ•°æ®
   - è¿ç§»å†å²æ•°æ®

3. **å›æ»šæ–¹æ¡ˆ**
   - ä¿ç•™æ—§ç³»ç»Ÿ
   - å¿«é€Ÿå›æ»šæœºåˆ¶
   - æ•°æ®ä¸€è‡´æ€§ä¿è¯

---

## ğŸ“‹ æ£€æŸ¥æ¸…å•

### å¼€å‘é˜¶æ®µ

- [ ] ä¾èµ–é›†æˆå®Œæˆ
- [ ] æ ¸å¿ƒæ¨¡å—å®ç°
- [ ] å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] APIæ–‡æ¡£å®Œæˆ
- [ ] ä»£ç å®¡æŸ¥é€šè¿‡

### æµ‹è¯•é˜¶æ®µ

- [ ] åŠŸèƒ½æµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡
- [ ] å‹åŠ›æµ‹è¯•é€šè¿‡
- [ ] å®‰å…¨æµ‹è¯•é€šè¿‡
- [ ] å…¼å®¹æ€§æµ‹è¯•é€šè¿‡

### éƒ¨ç½²é˜¶æ®µ

- [ ] éƒ¨ç½²æ–‡æ¡£å®Œæˆ
- [ ] ç›‘æ§é…ç½®å®Œæˆ
- [ ] å‘Šè­¦è§„åˆ™é…ç½®
- [ ] å¤‡ä»½æ–¹æ¡ˆå°±ç»ª
- [ ] å›æ»šæ–¹æ¡ˆå°±ç»ª

### ç”Ÿäº§é˜¶æ®µ

- [ ] ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [ ] ç›‘æ§æ­£å¸¸è¿è¡Œ
- [ ] æ€§èƒ½æŒ‡æ ‡æ­£å¸¸
- [ ] ç”¨æˆ·éªŒæ”¶é€šè¿‡
- [ ] æ–‡æ¡£æ›´æ–°å®Œæˆ

---

## ğŸ¯ æˆåŠŸæ ‡å‡†

### åŠŸèƒ½æ ‡å‡†

- âœ… æ”¯æŒè‡³å°‘5ç§æ¨¡å‹ç±»å‹ï¼ˆLLM/CV/Audioç­‰ï¼‰
- âœ… æ”¯æŒCPU/CUDA/Metalè®¾å¤‡
- âœ… APIå“åº”æ—¶é—´ < 100msï¼ˆä¸å«æ¨ç†ï¼‰
- âœ… æ¨¡å‹åŠ è½½æ—¶é—´ < 30s
- âœ… æ”¯æŒå¹¶å‘æ¨ç†ï¼ˆè‡³å°‘3ä¸ªå¹¶å‘ï¼‰

### æ€§èƒ½æ ‡å‡†

- âœ… æ¨ç†å»¶è¿Ÿ P50 < 1sï¼ˆå°æ¨¡å‹ï¼‰
- âœ… æ¨ç†å»¶è¿Ÿ P95 < 5sï¼ˆå¤§æ¨¡å‹ï¼‰
- âœ… ååé‡ > 100 req/sï¼ˆCPUï¼‰
- âœ… ååé‡ > 500 req/sï¼ˆGPUï¼‰
- âœ… å†…å­˜ä½¿ç”¨ < 8GBï¼ˆå•å®ä¾‹ï¼‰

### å¯é æ€§æ ‡å‡†

- âœ… å¯ç”¨æ€§ > 99.9%
- âœ… é”™è¯¯ç‡ < 0.1%
- âœ… æ•…éšœæ¢å¤æ—¶é—´ < 30s
- âœ… æ•°æ®ä¸€è‡´æ€§ 100%

---

## ğŸ“ æ”¯æŒå’Œè”ç³»

### æ–‡æ¡£èµ„æº

- [Candleå®˜æ–¹æ–‡æ¡£](https://huggingface.github.io/candle/)
- [Candle GitHub](https://github.com/huggingface/candle)
- [é¡¹ç›®Wiki](./docs/)

### æŠ€æœ¯æ”¯æŒ

- é—®é¢˜åé¦ˆ: GitHub Issues
- æŠ€æœ¯è®¨è®º: å›¢é˜ŸSlacké¢‘é“
- ç´§æ€¥æ”¯æŒ: è”ç³»é¡¹ç›®è´Ÿè´£äºº

---

## ğŸ“ é™„å½•

### A. æ¨¡å‹æ”¯æŒåˆ—è¡¨

| æ¨¡å‹ç±»å‹ | æ¨¡å‹åç§° | çŠ¶æ€ | å¤‡æ³¨ |
|---------|---------|------|------|
| LLM | LLaMA 7B/13B/70B | âœ… | æ”¯æŒé‡åŒ–ç‰ˆæœ¬ |
| LLM | Mistral 7B | âœ… | æ”¯æŒInstructç‰ˆæœ¬ |
| LLM | Phi 1.5/2/3 | âœ… | è½»é‡çº§æ¨¡å‹ |
| LLM | Gemma 2B/7B | âœ… | Googleæ¨¡å‹ |
| CV | YOLO v3/v8 | âœ… | ç›®æ ‡æ£€æµ‹ |
| CV | Segment Anything | âœ… | å›¾åƒåˆ†å‰² |
| CV | CLIP | âœ… | å¤šæ¨¡æ€ |
| Audio | Whisper | âœ… | è¯­éŸ³è¯†åˆ« |
| Audio | EnCodec | âœ… | éŸ³é¢‘å‹ç¼© |

### B. æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ

ï¼ˆå¾…è¡¥å……å®é™…æµ‹è¯•æ•°æ®ï¼‰

### C. å¸¸è§é—®é¢˜FAQ

**Q: å¦‚ä½•é€‰æ‹©è®¾å¤‡ç±»å‹ï¼Ÿ**
A: CPUé€‚åˆå°æ¨¡å‹å’Œå¼€å‘æµ‹è¯•ï¼ŒCUDAé€‚åˆå¤§æ¨¡å‹å’Œç”Ÿäº§ç¯å¢ƒï¼ŒMetalé€‚åˆAppleè®¾å¤‡ã€‚

**Q: æ¨¡å‹åŠ è½½å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ**
A: å¯ç”¨æ¨¡å‹ç¼“å­˜ï¼Œé¢„åŠ è½½å¸¸ç”¨æ¨¡å‹ï¼Œä½¿ç”¨SSDå­˜å‚¨æ¨¡å‹æ–‡ä»¶ã€‚

**Q: GPUå†…å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**
A: ä½¿ç”¨æ¨¡å‹é‡åŒ–ï¼Œå‡å°‘batch sizeï¼Œä½¿ç”¨å¤šGPUåˆ†å¸ƒå¼æ¨ç†ã€‚

**Q: å¦‚ä½•ç›‘æ§æ¨ç†æ€§èƒ½ï¼Ÿ**
A: ä½¿ç”¨PrometheusæŒ‡æ ‡ï¼ŒæŸ¥çœ‹Grafanaä»ªè¡¨æ¿ï¼Œåˆ†ææ—¥å¿—ã€‚

---

## ğŸ“… ç‰ˆæœ¬å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | ä½œè€… | è¯´æ˜ |
|------|------|------|------|
| 1.0.0 | 2024-01-XX | Edge Compute Team | åˆå§‹ç‰ˆæœ¬ï¼Œå®Œæ•´é›†æˆæ–¹æ¡ˆ |

---

**æ–‡æ¡£ç»“æŸ**

